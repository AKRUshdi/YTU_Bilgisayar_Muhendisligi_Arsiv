# -*- coding: utf-8 -*-
"""doğal dil eğitme kodu.ipnyb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LHjvy9S_YiqkkDsVngBTOPxsl-X4f2Fc
"""

import torch
import os
from torch.utils.data import DataLoader, Dataset, random_split
from torch.optim import AdamW
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.nn.utils.rnn import pad_sequence
from collections import Counter
import re
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import datetime
from tqdm import tqdm  # İlerleme çubuğu için tqdm kütüphanesini kullanıyoruz.

from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

lr = 5e-5
bs = 32

base_path = "/content/drive/My Drive/Doğal Dil Proje"
epoch_sayisi = 35
max_len = 128
# anahatlar
metin = "text"
label = "diabetes"
model_name = "ytu-ce-cosmos/turkish-gpt2-medium"
veri_seti_adi = "ana_veri_seti.csv"
#veri_seti_adi =  "ana_veri_seti_deneme.csv"
# Veri seti dosya yolu
data_filepath = os.path.join(base_path, veri_seti_adi)
# Dosya adları
model_adi = "BestModelParameters"
checkpoint_path_name = "checkpoint"
checkpoint_name = 'checkpoint.pth'
best_checkpoint_name = 'best_checkpoint.pth'
checkpoint_pth = os.path.join(base_path, checkpoint_path_name)
model_yolu = os.path.join(base_path, model_adi)
model_path = model_name.replace("/", "_").replace("-", "_")
model_path_without_label = os.path.join(base_path, model_path)
gorsel_klasor_adi = "gorseller"
gorsel_yolu = os.path.join(base_path, gorsel_klasor_adi)
SORU_TOKEN ="<soru>"
CEVAP_TOKEN ="<cevap>"

# Veri dosyasını oku
df = pd.read_csv(data_filepath, on_bad_lines='skip')

print(df.head().to_markdown())
len(df)

class QADataset(Dataset):
    def __init__(self, dataframe, tokenizer):
        self.tokenizer = tokenizer
        self.data = dataframe

        # Tokenizer için bir padding token'ı atayın, eğer yoksa
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.questions = self.data['text'].apply(lambda  x: SORU_TOKEN + x.split("Cevap:")[0].strip().replace("Soru:","") + SORU_TOKEN)
        self.answers = self.data['text'].apply(lambda  x: CEVAP_TOKEN + x.split("Cevap:")[1].strip() + CEVAP_TOKEN)

        self.inputs = []
        self.attention_masks = []

        for question, answer in zip(self.questions, self.answers):
            # Soru ve cevabı tokenizer ile encode edin
            encoded_pair = self.tokenizer.encode_plus(question, answer,
                                                      add_special_tokens=True,
                                                      max_length=max_len,
                                                      padding='max_length',
                                                      truncation=True,
                                                      return_tensors="pt")
            self.inputs.append(encoded_pair['input_ids'])
            self.attention_masks.append(encoded_pair['attention_mask'])

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        return {
            'input_ids': self.inputs[idx].squeeze(),  # Batch boyutunu kaldır
            'attention_mask': self.attention_masks[idx].squeeze()  # Batch boyutunu kaldır
        }

def calculate_perplexity(loss):
    return np.exp(loss)

# Özel collate_fn fonksiyonu
def collate_fn(batch):
    # Tokenizer'ın pad_token_id'sini kullanarak padding yapın
    input_ids = [item["input_ids"].squeeze(0) for item in batch]  # squeeze(0) boyutunu kaldırır
    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)

    # Dikkat maskesi oluşturun (padding olan yerlerde 0, diğer yerlerde 1)
    attention_mask = torch.zeros_like(input_ids_padded)
    attention_mask[input_ids_padded != tokenizer.pad_token_id] = 1

    return {"input_ids": input_ids_padded, "attention_mask": attention_mask}

def plot_metrics(losses, perplexities, lr, bs, epochs, gorsel_yolu = gorsel_yolu, gorsel_adi = "Normal"):
    os.makedirs(gorsel_yolu, exist_ok=True)  # Klasör yoksa
    # Kayıp grafiğini kaydetme yolu
    loss_graph_path = os.path.join(gorsel_yolu, f"{gorsel_adi}_lr_{lr}_bs_{bs}_loss.png")
    # Perplexity grafiğini kaydetme yolu
    perplexity_graph_path = os.path.join(gorsel_yolu, f"{gorsel_adi}_lr_{lr}_bs_{bs}_perplexity.png")

    epochs_range = range(1, epochs + 1)

    # Kayıp grafiği
    plt.figure(figsize=(6, 4))
    plt.plot(epochs_range, losses, label='Loss')
    plt.title(f'{gorsel_adi} Loss with lr={lr}, bs={bs}')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.tight_layout()
    plt.savefig(loss_graph_path)
    plt.close()  # Aktif figürü kapat

    # Perplexity grafiği
    plt.figure(figsize=(6, 4))
    plt.plot(epochs_range, perplexities, label='Perplexity')
    plt.title(f'{gorsel_adi} Perplexity with lr={lr}, bs={bs}')
    plt.xlabel('Epoch')
    plt.ylabel('Perplexity')
    plt.legend()
    plt.tight_layout()
    plt.savefig(perplexity_graph_path)
    plt.close()  # Aktif figürü kapat

def find_unique_words(texts):
    # Tüm metinlerdeki kelimeleri bulun ve sayın
    words = Counter(re.findall(r'\w+', ' '.join(texts)))
    return list(words.keys())

def save_checkpoint(model, optimizer, epoch, loss, losses, perplexities, checkpoint_path, checkpoint_name = checkpoint_name):
    if not os.path.exists(checkpoint_path):
        os.makedirs(checkpoint_path)
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'losses': losses,
        'perplexities': perplexities
    }
    torch.save(checkpoint, os.path.join(checkpoint_path, checkpoint_name))
    print(f"Checkpoint şuraya kaydedildi -> {checkpoint_path} loss değeri: {loss}")
def load_checkpoint(checkpoint_path, model, optimizer,checkpoint_name = checkpoint_name):
    checkpoint_filepath = os.path.join(checkpoint_path, checkpoint_name)
    if os.path.exists(checkpoint_filepath):
        print(f"Checkpoint şu konumdan yükleniyor ->  {checkpoint_filepath}")
        checkpoint = torch.load(checkpoint_filepath, map_location=torch.device(device))
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        epoch = checkpoint['epoch']
        loss = checkpoint.get('loss', float('inf'))
        losses = checkpoint.get('losses', [])
        perplexities = checkpoint.get('perplexities', [])
        val_losses = checkpoint.get('val_losses', [])
        val_perplexities = checkpoint.get('val_perplexities', [])
        return model, optimizer, epoch + 1, loss, losses, perplexities
    else:
        print("Hiçbir checkpoint bulunamadı, sıfırdan başlanıyor.")
        return model, optimizer, 0, float('inf'), [], []

def train(model, device, train_dataloader, checkpoint_path = checkpoint_pth):
    optimizer = AdamW(model.parameters(), lr=lr)

    # Checkpoint'ten modeli, optimizer'ı, epoch bilgisini, en iyi loss değerini ve metrik listelerini yükle
    model, optimizer, start_epoch, best_loss, losses, perplexities = load_checkpoint(checkpoint_path, model, optimizer)
    if start_epoch == epoch_sayisi:
        print("Eğitim zaten tamam...")
        return None
    # Epoch'ları döngüye al<ct
    for epoch in range(start_epoch, epoch_sayisi):
        print(f"Epoch {epoch} başlıyor...")
        model.train()
        total_loss = 0
        total_perplexity = 0
        for batch in tqdm(train_dataloader, desc=f"Epoch {epoch}", unit="batch"):
            optimizer.zero_grad()
            inputs = {key: value.to(device) for key, value in batch.items()}
            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss
            loss.backward()
            optimizer.step()

            batch_loss = loss.item()
            batch_perplexity = calculate_perplexity(batch_loss)  # Perplexity hesaplama fonksiyonu
            total_loss += batch_loss
            total_perplexity += batch_perplexity

        epoch_loss = total_loss / len(train_dataloader)
        epoch_perplexity = total_perplexity / len(train_dataloader)
        print(f"Epoch {epoch} | Loss: {epoch_loss:.4f} | Perplexity: {epoch_perplexity:.4f}")

        # Kaydedilen metrik listelerini ve en iyi modeli güncelle
        losses.append(epoch_loss)
        perplexities.append(epoch_perplexity)
        save_checkpoint(model, optimizer, epoch, best_loss, losses, perplexities, checkpoint_path, checkpoint_name)
        is_best = epoch_loss < best_loss
        best_loss = epoch_loss
    # Eğitim ve doğrulama metriklerinin grafiklerini çizdir
    plot_metrics(losses, perplexities, lr, len(train_dataloader.dataset), epoch_sayisi)

    return model

texts = df[metin].tolist()
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
model.to(device)

unique_words = find_unique_words(texts)

special_tokens = [SORU_TOKEN, CEVAP_TOKEN]
# Tokenizer'da olmayan özel tokenları bul
new_special_tokens = [token for token in special_tokens if token not in tokenizer.get_vocab()]


# Tokenizer'da olmayan kelimeleri tespit et
new_tokens = [word for word in unique_words if tokenizer.convert_tokens_to_ids(word) == tokenizer.unk_token_id]
if new_tokens:
    tokenizer.add_tokens(new_tokens)
    model.resize_token_embeddings(len(tokenizer))
if new_special_tokens:
    tokenizer.add_special_tokens({'additional_special_tokens': new_special_tokens})
    model.resize_token_embeddings(len(tokenizer))

dataset = QADataset(dataframe=df, tokenizer=tokenizer)

dataset

# DataLoader'ları oluştur
train_dataloader = DataLoader(dataset, batch_size=bs, shuffle=True)

model = train(model, device, train_dataloader)

if model is not None:
    # Model ve tokenizer kaydetme
    model.save_pretrained(model_yolu)
    dataset.tokenizer.save_pretrained(model_yolu)